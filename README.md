# hi-ai

## 《大模型之美》

### Note

我们始终没有摆脱“有多少人工就有多少智能”这样一个诅咒。

对着 OpenAI 发布的 Cookbook 做各种各样的尝试。

- 为什么人人都应该学习如何开发新一代 AI 应用？
  - 1 这一轮的 AI 浪潮里，开发新的 AI 应用的门槛大大降低了。
    - 过去需要有不错的数学基础，熟悉微积分、线性代数和概率论；然后掌握大量的机器学习和深度学习的知识，了解各种基础模型，比如逻辑回归、SVM、CNN、LSTM 等等的原理和实现；接着，你还要学会使用各种机器学习的编程框架，比如 TensorFlow 或者 PyTorch，买上一块价格不菲的 GPU 尝试训练模型；最后，你还需要理解在实际应用里锤炼机器学习的各种实战技巧和模型，比如各种各样的特征工程方式、Dropout 等正则化方法、超参数调优等等。
  - 2 这一轮的 AI 浪潮里，对应技术能够应用的范围非常广泛，可以说是包罗万象。
    - 开始让我们看见了“通用人工智能”（AGI）的雏形
  - 3 这个浪潮带来的变化会对我们每一个人的工作带来巨大的冲击。
  
这两个例子，其实对应着很多不同的问题，其中就包括机器翻译、文本生成、知识推理、命名实体识别等等。在传统的机器学习领域，对于其中任何一个问题，都可能需要一个独立的机器学习模型。就算把这些模型都免费提供给你，把这些独立的机器学习模型组合到一起实现上面的效果，还需要海量的工程研发工作。没有一个数十人的团队，工作量根本看不到头。 👍🏻 🐂

对 OpenAI 提供的大语言模型能干什么有了一个最直观的认识，GPT-3.5 系列的大语言模型，可以让你使用一个模型来解决所有的自然语言处理问题。原先我们需要一个个地单独训练模型，或者至少需要微调模型的场景，在大语言模型之下都消失了。

原来需要有相对丰富的机器学习经验，我们需要将数据集切分成训练（Training）、验证（Validation）、测试（Test）三组数据，然后通过 AUC 或者混淆矩阵（Confusion Matrix）来衡量效果。如果数据量不够多，为了训练效果的稳定性，可能需要采用 K-Fold 的方式来进行训练。因为我们有了大语言模型，可以通过它提供的 Completion 和 Embedding 这两个 API，用不到 10 行代码就能完成情感分析，并且能获得非常好的效果。

我们利用不同文本在大语言模型里 Embedding 之间的距离，来进行情感分析。这种使用大语言模型的技巧，一般被称做零样本分类。所谓零样本分类，也就是我们不需要任何新的样本来训练机器学习的模型，就能进行分类。我们认为，之前经过预训练的大语言模型里面，已经蕴含了情感分析的知识。

## 《ChatGPT & LLMs - Practical Guide》
